{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# VGG-16","metadata":{"id":"DOV-7BbZAHFv"}},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{"id":"XvJ9ZkdyALzW"}},{"cell_type":"code","source":"%matplotlib inline\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom tqdm import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sn\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Device Setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"AlNF5q7bALPf","execution":{"iopub.status.busy":"2023-06-22T14:55:45.519729Z","iopub.execute_input":"2023-06-22T14:55:45.520217Z","iopub.status.idle":"2023-06-22T14:55:45.531122Z","shell.execute_reply.started":"2023-06-22T14:55:45.520171Z","shell.execute_reply":"2023-06-22T14:55:45.529998Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Loading the CIFAR-10 Dataset","metadata":{"id":"82m49XTyCKl2"}},{"cell_type":"code","source":"def load_train_val( data_dir,\n                    batch_size,\n                    val_batch_size,\n                    random_seed,\n                    augment,\n                    val_size=0.1,\n                    shuffle=True):\n\n  normalize = transforms.Normalize(\n        mean=[0.4913997551666284, 0.48215855929893703, 0.4465309133731618],\n        std=[0.24703225141799082, 0.24348516474564, 0.26158783926049628],\n  )\n\n  # Transform\n  transform = transforms.Compose([\n      transforms.Resize((227,227)),\n      transforms.ToTensor(),\n      normalize,\n  ])\n\n  if augment:\n    train_transform = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        normalize,\n    ])\n  else:\n    train_transform = transforms.Compose([\n        transforms.Resize((227,227)),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n\n  # load the dataset\n  train_dataset = datasets.CIFAR10(\n      root=data_dir, train=True,\n      download=True, transform=train_transform,\n  )\n\n  val_dataset = datasets.CIFAR10(\n      root=data_dir, train=True,\n      download=True, transform=transform,\n  )\n\n  num_train = len(train_dataset)\n  indices = list(range(num_train))\n  split = int(np.floor(val_size * num_train))\n  if shuffle:\n      np.random.seed(random_seed)\n      np.random.shuffle(indices)\n\n  train_idx, val_idx = indices[split:], indices[:split]\n  train_sampler = SubsetRandomSampler(train_idx)\n  val_sampler = SubsetRandomSampler(val_idx)\n\n  train_loader = torch.utils.data.DataLoader(\n      train_dataset, batch_size=batch_size, sampler=train_sampler)\n\n  val_loader = torch.utils.data.DataLoader(\n      val_dataset, batch_size=val_batch_size, sampler=val_sampler)\n\n  return (train_loader, val_loader)\n\ndef load_test(data_dir,\n              batch_size,\n              shuffle=True):\n\n  normalize = transforms.Normalize(\n        mean=[0.4913997551666284, 0.48215855929893703, 0.4465309133731618],\n        std=[0.24703225141799082, 0.24348516474564, 0.26158783926049628],\n  )\n\n  # Transform\n  transform = transforms.Compose([\n      transforms.Resize((227,227)),\n      transforms.ToTensor(),\n      normalize,\n  ])\n\n  # load the dataset\n  test_dataset = datasets.CIFAR10(\n      root=data_dir, train=True,\n      download=True, transform=transform,\n  )\n\n  data_loader = torch.utils.data.DataLoader(\n      test_dataset, batch_size=batch_size, shuffle=shuffle\n  )\n\n  return data_loader","metadata":{"id":"QzxWpA6iCRoH","execution":{"iopub.status.busy":"2023-06-22T14:55:45.533543Z","iopub.execute_input":"2023-06-22T14:55:45.534252Z","iopub.status.idle":"2023-06-22T14:55:45.553393Z","shell.execute_reply.started":"2023-06-22T14:55:45.534216Z","shell.execute_reply":"2023-06-22T14:55:45.552001Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# reduced batch size from the original by half cause of memory issues\ntrain_loader, val_loader = load_train_val(data_dir = './data', batch_size = 64, val_batch_size=32, augment=False, random_seed = 1)\ntest_loader = load_test(data_dir = './data', batch_size = 64)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5C7WiBPbHUho","outputId":"ad4b3301-98ba-45b2-80de-1e7c477dbcb4","execution":{"iopub.status.busy":"2023-06-22T14:55:45.557598Z","iopub.execute_input":"2023-06-22T14:55:45.558109Z","iopub.status.idle":"2023-06-22T14:55:48.717162Z","shell.execute_reply.started":"2023-06-22T14:55:45.558070Z","shell.execute_reply":"2023-06-22T14:55:48.715827Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## VGGNET Model","metadata":{"id":"rxOB-DSfK6ro"}},{"cell_type":"code","source":"class VGG16(nn.Module):\n    def __init__(self, num_classes=10):\n        super(VGG16, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU())\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(), \n            nn.MaxPool2d(kernel_size = 2, stride = 2))\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU())\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2, stride = 2))\n        self.layer5 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU())\n        self.layer6 = nn.Sequential(\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU())\n        self.layer7 = nn.Sequential(\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2, stride = 2))\n        self.layer8 = nn.Sequential(\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU())\n        self.layer9 = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU())\n        self.layer10 = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2, stride = 2))\n        self.layer11 = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU())\n        self.layer12 = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU())\n        self.layer13 = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2, stride = 2))\n        self.fc = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(7*7*512, 4096),\n            nn.ReLU())\n        self.fc1 = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU())\n        self.fc2= nn.Sequential(\n            nn.Linear(4096, num_classes))\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.layer5(out)\n        out = self.layer6(out)\n        out = self.layer7(out)\n        out = self.layer8(out)\n        out = self.layer9(out)\n        out = self.layer10(out)\n        out = self.layer11(out)\n        out = self.layer12(out)\n        out = self.layer13(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc(out)\n        out = self.fc1(out)\n        out = self.fc2(out)\n        return out","metadata":{"id":"nkOV_LsyK_bA","execution":{"iopub.status.busy":"2023-06-22T14:55:48.720273Z","iopub.execute_input":"2023-06-22T14:55:48.720698Z","iopub.status.idle":"2023-06-22T14:55:48.746410Z","shell.execute_reply.started":"2023-06-22T14:55:48.720662Z","shell.execute_reply":"2023-06-22T14:55:48.744417Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Set HyperParams","metadata":{"id":"oM4jqK9RM15B"}},{"cell_type":"code","source":"num_classes = 10\nnum_epochs = 25 # 100 took way too long\nbatch_size = 128\nlearning_rate = 0.005\n\n# constant for classes\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\n\n# Adding weights using normal distribution\ndef init_weights(model):\n  if (isinstance(model, nn.Conv2d) or isinstance(model, nn.Linear)):\n    model.weight.data.normal_(0, 0.01)\n    model.bias.data.fill_(0.)\n\nmodel = VGG16(num_classes).to(device)\nmodel.apply(init_weights)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n\n# Training length\ntotal_step = len(train_loader)\n\n#Tensorboard Setup\ntb = SummaryWriter(\"runs/AlexNet\")\nimages, labels = next(iter(train_loader))\ngrid = torchvision.utils.make_grid(images)\ntb.add_image(\"images\", grid)","metadata":{"id":"l70s93JIN9Dw","execution":{"iopub.status.busy":"2023-06-22T14:55:48.748411Z","iopub.execute_input":"2023-06-22T14:55:48.749493Z","iopub.status.idle":"2023-06-22T14:55:57.209503Z","shell.execute_reply.started":"2023-06-22T14:55:48.749457Z","shell.execute_reply":"2023-06-22T14:55:57.172479Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{"id":"5gys9JHJcnU1"}},{"cell_type":"code","source":"total_steps = len(train_loader)\n\nfor epoch in tqdm(range(num_epochs)):\n    for i, (images, labels) in enumerate(train_loader):\n        # Move tensors to the configured device\n        images = images.to(device)\n        labels = labels.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    #if (epoch+1) % 10 == 0:\n    #  print (f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_steps}], Loss: {loss.item():.4f}\")\\\n    \n    y_pred = [] # save predction\n    y_true = [] # save ground truth\n    # Validation\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for images, labels in val_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            y_pred.extend(predicted.data.cpu().numpy())\n            y_true.extend(labels.data.cpu().numpy())\n            del images, labels, outputs\n        #if(epoch+1) % 10 == 0:\n        #  print(f\"Accuracy of the network on the {5000} validation images: {100*correct/total} %\")\n    tb.add_scalar(\"Loss\", loss, epoch)\n    tb.add_scalar(\"Correct\", correct, epoch)\n    tb.add_scalar(\"Accuracy\", correct/total, epoch)\n    # Build confusion matrix\n    cf_matrix = confusion_matrix(y_true, y_pred)\n    df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index=[i for i in classes],\n                         columns=[i for i in classes])\n    plt.figure(figsize=(12, 7)) \n    tb.add_figure(\"Confusion matrix\", sn.heatmap(df_cm, annot=True).get_figure(), epoch)\n    for name, weight in model.named_parameters():\n      tb.add_histogram(name,weight, epoch)\n      tb.add_histogram(f'{name}.grad',weight.grad, epoch)","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"i3xlHEEWcmTO","outputId":"ae179036-4794-459a-cfa0-142405ccceb8","execution":{"iopub.status.busy":"2023-06-22T14:55:57.211357Z","iopub.execute_input":"2023-06-22T14:55:57.211764Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":" 64%|██████▍   | 16/25 [2:01:02<1:08:07, 454.20s/it]","output_type":"stream"}]},{"cell_type":"markdown","source":"## Testing","metadata":{"id":"JS8Y8m_cdEC4"}},{"cell_type":"code","source":"with torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in tqdm(test_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        del images, labels, outputs\n    print(f\"Accuracy of the network on the {10000} test images: {100*correct/total} %\")","metadata":{"id":"8jhKBsa4dFfA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tensorboard","metadata":{"id":"6TJjsQKk1PAS"}},{"cell_type":"code","source":"tb.flush()\ntb.close()\n%load_ext tensorboard\n%tensorboard --logdir=runs/AlexNet","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}